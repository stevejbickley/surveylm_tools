coerce_final_columns <- function(df) {
df$critic.prompt <- as.character(df$critic.prompt)
df$analysis <- as.character(df$analysis)
df$suggestions <- as.character(df$suggestions)
df$list1 <- as.character(df$list1)
df$list2 <- as.character(df$list2)
df$extra_answers <- as.character(df$extra_answers)
return(df)
}
# ---- SETUP
# i) Set file paths for different gpt-based studies
# Journal of Services Marketing (JSM)
#jsmwd <- "/Users/stevenbickley/Library/CloudStorage/Dropbox/chatGPT-research-QUT/doc/journal-of-services-marketing/data/output/Soderlund_&_Oikarien_2018" # old
#jsmheadwd <- "/Users/stevenbickley/Library/CloudStorage/Dropbox/chatGPT-research-QUT/doc/journal-of-services-marketing/data/output" # old
jsmwd <- "/Users/stevenbickley/stevejbickley/SurveyLM_Agent_Profile_Generator/simulation_outputs"
jsmheadwd <- "/Users/stevenbickley/stevejbickley/SurveyLM_Agent_Profile_Generator/"
# WHU Delphi Study
whus1wd <- "/Users/stevenbickley/Library/CloudStorage/Dropbox/chatGPT-research-QUT/doc/delphi_sports_study/data/outputs/product"
whus2wd <- "/Users/stevenbickley/Library/CloudStorage/Dropbox/chatGPT-research-QUT/doc/delphi_sports_study/data/outputs/production"
whuheadwd <- "/Users/stevenbickley/Library/CloudStorage/Dropbox/chatGPT-research-QUT/doc/delphi_sports_study/data/outputs"
#whutestingwd <-"/Users/stevenbickley/Library/CloudStorage/Dropbox/chatGPT-research-QUT/doc/delphi_sports_study/data/outputs/testing"
# Michael Guihot Technology Platforms Power Surveys/Questionnaires
mgtechwd <- "/Users/stevenbickley/Library/CloudStorage/Dropbox/chatGPT-research-QUT/doc/MGuihot_Online_Power/data/outputs"
mgtechheadwd <- "/Users/stevenbickley/Library/CloudStorage/Dropbox/chatGPT-research-QUT/doc/MGuihot_Online_Power/data/outputs"
#mgtechtestingwd <- "/Users/stevenbickley/Library/CloudStorage/Dropbox/chatGPT-research-QUT/doc/MGuihot_Online_Power/data/outputs/testing"
# NOTE:
# 1) File path for JSM vignette study: "/Users/stevenbickley/Library/CloudStorage/Dropbox/chatGPT-research-QUT/doc/journal-of-services-marketing/data/output/Soderlund_&_Oikarien_2018"
# 2) File path for WHU Delphi study 1 (whus1) - Product: "/Users/stevenbickley/Library/CloudStorage/Dropbox/chatGPT-research-QUT/doc/delphi_sports_study/data/outputs/product"
# 3) File path for WHU Delphi study 2 (whus2) - Production: "/Users/stevenbickley/Library/CloudStorage/Dropbox/chatGPT-research-QUT/doc/delphi_sports_study/data/outputs/production"
# a) Set current working directory
currwd <- jsmwd
setwd(currwd)
headwd <- jsmheadwd
# b) Retrieve list of all simulation data files
allfiles <- list.files(
path = currwd, # replace with the directory you want
pattern = "sbickley1_completed_survey_data.*\\.csv$", # filenames ending in "_matches.csv"
full.names = TRUE, # include the directory in the result
recursive = TRUE # scan all subdirectories
)
# c1) Filter out files from 'superseded' and 'old' directories
allfiles <- allfiles[!grepl("/superseded/", allfiles)]
allfiles <- allfiles[!grepl("/old/", allfiles)]
### -------------- STEP 2A (ONE STUDY): READ IN SIMULATION DATA, MERGE INTO ONE DATAFRAME, CONVERT TO WIDE FORMAT --------------
# NOTE: need to change "allfiles_study1" below to "allfiles" only....
# Lazy way at the moment...
allfiles_study1=allfiles
# -- 0a) Read all csv files then perform row-binding with dplyr
list_of_dataframes_study1 <- lapply(allfiles_study1, read.csv)
# -- 0b) Apply the coercion function to each dataframe in the list
list_of_dataframes_study1 <- lapply(list_of_dataframes_study1, coerce_columns_jsm) # NOTE: coerce_columns_jsm OR coerce_columns_whu
# -- 0c) Bind rows together
combined_df_study1 <- bind_rows(list_of_dataframes_study1)
# -- 0d) Aggregate data by 'agent' and 'question.id' and create 'multiple_rows_answered'
aggregated_df_study1 <- combined_df_study1 %>%
group_by(`simulation.id`, agent, `question.id`) %>%
mutate(answer = paste0("[", paste(answer, collapse = ","), "]"),
multiple_rows_answered = ifelse(n() > 1, 1, 0)) %>%
slice(1) %>%
ungroup()
# -- 0e) Remove double opening and closing square brackets
aggregated_df_study1$answer <- gsub("\\[\\[", "[", aggregated_df_study1$answer)
aggregated_df_study1$answer <- gsub("\\]\\]", "]", aggregated_df_study1$answer)
# -- 0f) Initialize columns with NA
aggregated_df_study1$list1 <- NA
aggregated_df_study1$list2 <- NA
# -- 0g) Function to handle aggregated answers
handle_aggregated_answer <- function(answer) {
# Check if answer needs to be wrapped in square brackets
if (startsWith(answer, "[") && grepl("\\],\\[", answer) && endsWith(answer, "]")) {
answer <- paste0("[", answer, "]")
}
# Try parsing, if it fails return the answer unmodified
parsed <- tryCatch({
fromJSON(answer, simplifyVector = FALSE)
}, error = function(e) NULL)
# If parsing failed or result is not a list of lists with exactly two members, return the original answer
if (is.null(parsed) || !is.list(parsed) || length(parsed) != 2 || !all(sapply(parsed, is.list))) {
return(list(answer = answer, list1 = NA, list2 = NA))
}
# Separate the two lists
list1 <- unlist(parsed[[1]])
list2 <- unlist(parsed[[2]])
# Check if the two lists have the same length
if(length(list1) != length(list2)) {
return(list(answer = answer, list1 = list1, list2 = list2))
}
# Compute the average
avg <- mapply(function(x, y) (as.numeric(x) + as.numeric(y)) / 2, list1, list2)
# Convert the average list to the desired character format
avg_string <- paste0("[", paste(avg, collapse = ", "), "]")
return(list(answer = avg_string, list1 = list1, list2 = list2))
}
# -- 0h) Apply the function to the 'answer' column, and bind the list of lists together
result_study1 <- lapply(aggregated_df_study1$answer, handle_aggregated_answer)
# -- 0i) Update the dataframe with the results
aggregated_df_study1$answer <- sapply(result_study1, function(x) x$answer)
aggregated_df_study1$list1 <- sapply(result_study1, function(x) x$list1)
aggregated_df_study1$list2 <- sapply(result_study1, function(x) x$list2)
# -- a) Parse the "answer" column # NOTE - UP TO HERE....
parsed_data_study1 <- lapply(aggregated_df_study1$answer, function(x) {
tryCatch({
fromJSON(x, simplifyVector = FALSE)
}, error = function(e) NULL)
})
# -- b) Determine the maximum length of answers across all rows
max_length_study1 <- max(sapply(parsed_data_study1, length))
# -- c) Extract answers according to the structure of each parsed object
parsed_answers_study1 <- lapply(parsed_data_study1, function(x) {extract_answers(x, max_length_study1)})
# -- d) Ensure Each "answers" is of length max_length - padded with NAs
parsed_answers_study1 <- lapply(parsed_answers_study1, function(item) {
item$answers <- c(item$answers, rep(NA, max_length_study1 - length(item$answers)))
return(item)
})
# -- e) Convert the parsed answers into a data.frame and add response_type column
# Step 1: Create the main part of answer_df_study2
answer_df_study1 <- lapply(parsed_answers_study1, function(x) {
answers <- x[["answers"]]
# Limit to max_length_study2, fill with NA if less
length(answers) <- max_length_study1
return(answers)
})
# Convert to a data frame
answer_df_study1 <- do.call(rbind, lapply(answer_df_study1, function(x) as.data.frame(t(x))))
colnames(answer_df_study1) <- paste0("answer_", seq_len(max_length_study1))
# Step 2: Create a separate column for extra answers
extra_answers <- lapply(parsed_answers_study1, function(x) {
answers <- x[["answers"]]
if(length(answers) > max_length_study1) {
return(answers[(max_length_study1 + 1):length(answers)])
} else {
return(NA)
}
})
answer_df_study1$extra_answers <- I(extra_answers) # 'I' to keep the list structure in the dataframe
# Step 3: Add response_type
response_types <- lapply(parsed_answers_study1, `[[`, "response_type")
answer_df_study1$response_type <- unlist(response_types)
# -- f) Bind the original dataframe with the answer_df
final_df_study1 <- bind_cols(aggregated_df_study1, answer_df_study1)
# Change response_type to 'two_lists' for rows where list1 or list2 are non-NA
final_df_study1$response_type[!is.na(final_df_study1$list1) | !is.na(final_df_study1$list2)] <- "two_lists"
# Function to check for list class in a column
is_list_in_column <- function(column) {
sapply(column, function(x) class(x) == "list")
}
# -- g) Extract answer instructions for each row
parsed_instructions_study1 <- lapply(aggregated_df_study1$`answer.instruction`, extract_instructions)
# -- h) Determine the max number of instructions across rows
max_instructions_study1 <- max(sapply(parsed_instructions_study1, length))
# -- i) Convert the list to a dataframe with wide format
# Step 1: Initialize a new list to store the modified instructions
modified_instructions_study1 <- vector("list", length = length(parsed_instructions_study1))
# Step 2: Ensure each element is a list of length 17
modified_instructions_study1 <- lapply(parsed_instructions_study1, function(x) {
# If the element is not a list, convert it to a list
if (is.character(x)){
x <- unlist(strsplit(x, "\\d+\\.\\s*"))
x <- unique(x[nzchar(x)])
x <- as.list(x)
} else if (!is.list(x)) {
x <- list(x)
}
# Pad the list with NAs to make its length 17
length(x) <- max_instructions_study1
return(x)
})
# Step 3:Combine all lists into a data frame
instruction_df_study1 <- do.call(rbind, modified_instructions_study1)
colnames(instruction_df_study1) <- paste0("instruction_", 1:max_instructions_study1)
# Step 4: Convert the result into a data frame
instruction_df_study1 <- as.data.frame(instruction_df_study1)
# -- j) Combine the new instruction columns with the original dataframe
final_df_study1 <- bind_cols(final_df_study1, instruction_df_study1)
# -- k1) Get the subset of columns that start with "answer_" and "instruction_" and convert to numeric and character, respectively
answer_columns_study1 <- grep("^answer_", colnames(final_df_study1), value = TRUE)
#final_df_study1[, answer_columns_study1] <- lapply(final_df_study1[, answer_columns_study1], as.numeric) # Convert those columns to numeric --> issues with list types at the moment??
instruction_columns_study1 <- grep("^instruction_", colnames(final_df_study1), value = TRUE)
final_df_study1[, instruction_columns_study1] <- lapply(final_df_study1[, instruction_columns_study1], as.character) # Convert those columns to character
# -- k2) Change the response_type where multiple_rows_answered is greater than 0, and Drop the multiple_rows_answered column
final_df_study1$response_type[final_df_study1$multiple_rows_answered > 0] <- "multiple_lines"
final_df_study1$multiple_rows_answered <- NULL
# -- li) Apply the coercion function to each dataframe in the list
final_df_study1 <- coerce_final_columns(final_df_study1)
# -- l1) Calculate the number of non-NA values for each row in the subset of columns, and save to xlsx file (OPTIONAL)
final_df_study1$non_na_counts <- apply(final_df_study1[, answer_columns_study1], MARGIN = 1, function(x) sum(!is.na(x)))
# (OPTIONAL) Count the number of occurrences for each unique value of "question.id" to check we have enough data coverage across the conditions/each experiment (i.e. greater than 500 observations, plu or minus 10%)
table(final_df_study1$question.id)
rm(list = ls())
#install.packages("httr")
library("httr")
library("data.table")
library("dplyr")
library("lubridate")
library("writexl")
library("jsonlite")
library("readxl")
library("stringr")
library("tidyr")
### -------------- STEP 1: DEFINE FUNCTIONS and SETUP CODE --------------
# ---- FUNCTIONS
# -- a) Function to extract answers and return response type - handles 4 different data structures in "answer" column
extract_answers <- function(data, max_length) {
response <- list(answers = NULL, response_type = NA)
# Check if the data is non-existent or empty
if (length(data)==0) {
response$answers <- c(NA, rep(NA, max_length - 1))
response$response_type <- "nil_response"
}
# Check if only a single answer is provided
else if (length(data)==1) {
response$answers <- c(data, rep(NA, max_length - 1))
response$response_type <- "single_answer"
}
# Check if the data is a named list (i.e., contains "id" and "answer")
else if ("id" %in% tolower(names(data)) && "answer" %in% tolower(names(data))) {
response$answers <- data$answer
response$response_type <- "named_list"
}
# Check if the data is a list of lists with nested "answer" values
else if (is.list(data[[1]]) && "answer" %in% tolower(names(data[[1]]))) {
response$answers <- unlist(lapply(data, function(x) x$answer))
response$response_type <- "list_of_lists"
}
# Handle single character cases
else if (is.character(data) || is.numeric(data)) {
response$answers <- c(data, rep(NA, max_length - 1))
response$response_type <- "single_character"
}
# Otherwise, assume the data is a simple list of answers
else {
response$answers <- data
response$response_type <- "simple_list"
}
return(response)
}
# -- b) # Function to extract answer instructions
extract_instructions <- function(instruction_str) {
# Split the string by newline
lines <- unlist(strsplit(instruction_str, split = "\n"))
# Use regex to extract the actual instruction part after the numbering
instructions <- sapply(lines, function(x) {
if (grepl("^\\d+\\. ", x)) {
sub("^\\d+\\. ", "", x)
} else if (is.na(x)){
NA
} else {
x # Return the original line if it doesn't match the pattern
}
})
# Remove NAs (lines that didn't match the instruction pattern)
return(instructions[!is.na(instructions)])
}
# -- c) # Function to each column to the desired datatype - JSM Vignette Study Only
coerce_columns_jsm <- function(df) {
df$simulation.id <- as.factor(df$simulation.id)
df$user.id <- as.factor(df$user.id)
df$status <- as.factor(df$status)
df$created <- as.character(df$created)
df$agent <- as.integer(df$agent)
df$question <- as.factor(df$question)
df$question.id <- as.factor(df$question.id)
df$answer.instruction <- as.character(df$answer.instruction)
df$answer <- as.character(df$answer)
df$justification <- as.factor(df$justification)
df$justification.prompt <- as.character(df$justification.prompt)
df$justification.content <- as.character(df$justification.content)
df$critic <- as.factor(df$critic)
df$critic.prompt <- as.character(df$critic.prompt)
df$analysis <- as.character(df$analysis)
df$suggestions <- as.character(df$suggestions)
df$model <- as.factor(df$model)
df$role <- as.factor(df$role)
df$temperature <- as.numeric(df$temperature)
return(df)
}
# -- d) # Function to each column to the desired datatype - WHU Delphi Study Only
coerce_columns_whu <- function(df) {
df$simulation.id <- as.factor(df$simulation.id)
df$user.id <- as.factor(df$user.id)
df$status <- as.factor(df$status)
df$created <- as.character(df$created)
#df$average.prompt.tokens <- as.integer(df$average.prompt.tokens)
#df$average.completion.tokens <- as.integer(df$average.completion.tokens)
#df$average.actual.costs <- as.integer(df$average.actual.costs)
df$agent <- as.integer(df$agent)
df$question <- as.factor(df$question)
df$question.id <- as.factor(df$question.id)
#df$number.of.questions <- as.integer(df$number.of.questions)
df$answer.instruction <- as.character(df$answer.instruction)
df$answer <- as.character(df$answer)
df$justification <- as.factor(df$justification)
df$justification.prompt <- as.character(df$justification.prompt)
df$justification.content <- as.character(df$justification.content)
df$critic <- as.factor(df$critic)
df$critic.prompt <- as.character(df$critic.prompt)
df$analysis <- as.character(df$analysis)
df$suggestions <- as.character(df$suggestions)
df$model <- as.factor(df$model)
df$role <- as.factor(df$role)
df$temperature <- as.numeric(df$temperature)
df$Age.group <- as.factor(df$Age.group)
df$Gender <- as.factor(df$Gender)
df$Nationality <- as.factor(df$Nationality)
df$Org.type <- as.factor(df$Please.indicate.the.type.of.organization...industry.you.work.for.)
df$Org.type.other <- as.character(df$Please.indicate.the.type.of.organization...industry.you.work.for...if.other.)
#df$Expert.sports.content <- as.factor(ifelse(df$Please.indicate.your.level.of.expertise.with.respect.to.top.tier.sports.content.consumption. %in% c("", "None",NaN), "NAN", df$Please.indicate.your.level.of.expertise.with.respect.to.top.tier.sports.content.consumption.))
#df$Expert.sports.content <- as.factor(df$Please.indicate.your.level.of.expertise.with.respect.to.top.tier.sports.content.consumption.)
df$Technical.details.greater.detail <- as.factor(df$Please.indicate.your.level.of.agreement.with.the.following.statements..I.like.to.occupy.myself.in.greater.detail.with.technical.systems)
df$Technical.details.do.not.care <- as.factor(df$Please.indicate.your.level.of.agreement.with.the.following.statements..It.is.enough.for.me.that.a.technical.system.works..I.do.not.care.how.or.why)
df$Technical.details.basic.function <- as.factor(df$Please.indicate.your.level.of.agreement.with.the.following.statements..It.is.enough.for.me.to.know.the.basic.function.of.a.technical.system)
df$Testing.new.tech <- as.factor(df$Please.indicate.your.level.of.agreement.with.the.following.statements..I.like.testing.the.functions.of.a.new.technical.system)
df$Handles.stress.well <- as.factor(df$Please.indicate.your.level.of.agreement.with.the.following.statements..I.see.myself.as.someone.who.is.relaxed.and.handles.stress.well)
df$Few.artistic.interests <- as.factor(df$Please.indicate.your.level.of.agreement.with.the.following.statements..I.see.myself.as.someone.who.has.few.artistic.interests)
df$Gets.nervous.easily <- as.factor(df$Please.indicate.your.level.of.agreement.with.the.following.statements..I.see.myself.as.someone.who.gets.nervous.easily)
df$Has.active.imagination <- as.factor(df$Please.indicate.your.level.of.agreement.with.the.following.statements..I.see.myself.as.someone.who.has.an.active.imagination)
tryCatch({
df$Expert.sports.content <- as.factor(ifelse(df$Please.indicate.your.level.of.expertise.with.respect.to.top.tier.sports.content.consumption. %in% c("", "None", "NaN", "NA"), "NAN", df$Please.indicate.your.level.of.expertise.with.respect.to.top.tier.sports.content.consumption.))
}, error = function(e) {
df$Expert.sports.content <- "NAN - invalid"
})
return(df)
}
# -- e) # Function to each column to the desired datatype - Relevant across all studies
coerce_final_columns <- function(df) {
df$critic.prompt <- as.character(df$critic.prompt)
df$analysis <- as.character(df$analysis)
df$suggestions <- as.character(df$suggestions)
df$list1 <- as.character(df$list1)
df$list2 <- as.character(df$list2)
df$extra_answers <- as.character(df$extra_answers)
return(df)
}
# ---- SETUP
# i) Set file paths for different gpt-based studies
# Journal of Services Marketing (JSM)
#jsmwd <- "/Users/stevenbickley/Library/CloudStorage/Dropbox/chatGPT-research-QUT/doc/journal-of-services-marketing/data/output/Soderlund_&_Oikarien_2018" # old
#jsmheadwd <- "/Users/stevenbickley/Library/CloudStorage/Dropbox/chatGPT-research-QUT/doc/journal-of-services-marketing/data/output" # old
jsmwd <- "/Users/stevenbickley/stevejbickley/SurveyLM_Agent_Profile_Generator/simulation_outputs"
jsmheadwd <- "/Users/stevenbickley/stevejbickley/SurveyLM_Agent_Profile_Generator/"
# WHU Delphi Study
whus1wd <- "/Users/stevenbickley/Library/CloudStorage/Dropbox/chatGPT-research-QUT/doc/delphi_sports_study/data/outputs/product"
whus2wd <- "/Users/stevenbickley/Library/CloudStorage/Dropbox/chatGPT-research-QUT/doc/delphi_sports_study/data/outputs/production"
whuheadwd <- "/Users/stevenbickley/Library/CloudStorage/Dropbox/chatGPT-research-QUT/doc/delphi_sports_study/data/outputs"
#whutestingwd <-"/Users/stevenbickley/Library/CloudStorage/Dropbox/chatGPT-research-QUT/doc/delphi_sports_study/data/outputs/testing"
# Michael Guihot Technology Platforms Power Surveys/Questionnaires
mgtechwd <- "/Users/stevenbickley/Library/CloudStorage/Dropbox/chatGPT-research-QUT/doc/MGuihot_Online_Power/data/outputs"
mgtechheadwd <- "/Users/stevenbickley/Library/CloudStorage/Dropbox/chatGPT-research-QUT/doc/MGuihot_Online_Power/data/outputs"
#mgtechtestingwd <- "/Users/stevenbickley/Library/CloudStorage/Dropbox/chatGPT-research-QUT/doc/MGuihot_Online_Power/data/outputs/testing"
# NOTE:
# 1) File path for JSM vignette study: "/Users/stevenbickley/Library/CloudStorage/Dropbox/chatGPT-research-QUT/doc/journal-of-services-marketing/data/output/Soderlund_&_Oikarien_2018"
# 2) File path for WHU Delphi study 1 (whus1) - Product: "/Users/stevenbickley/Library/CloudStorage/Dropbox/chatGPT-research-QUT/doc/delphi_sports_study/data/outputs/product"
# 3) File path for WHU Delphi study 2 (whus2) - Production: "/Users/stevenbickley/Library/CloudStorage/Dropbox/chatGPT-research-QUT/doc/delphi_sports_study/data/outputs/production"
# a) Set current working directory
currwd <- jsmwd
setwd(currwd)
headwd <- jsmheadwd
# b) Retrieve list of all simulation data files
allfiles <- list.files(
path = currwd, # replace with the directory you want
pattern = "sbickley1_completed_survey_data.*\\.csv$", # filenames ending in "_matches.csv"
full.names = TRUE, # include the directory in the result
recursive = TRUE # scan all subdirectories
)
# c1) Filter out files from 'superseded' and 'old' directories
allfiles <- allfiles[!grepl("/superseded/", allfiles)]
allfiles <- allfiles[!grepl("/old/", allfiles)]
### -------------- STEP 2A (ONE STUDY): READ IN SIMULATION DATA, MERGE INTO ONE DATAFRAME, CONVERT TO WIDE FORMAT --------------
# NOTE: need to change "allfiles_study1" below to "allfiles" only....
# Lazy way at the moment...
allfiles_study1=allfiles
# -- 0a) Read all csv files then perform row-binding with dplyr
list_of_dataframes_study1 <- lapply(allfiles_study1, read.csv)
# -- 0b) Apply the coercion function to each dataframe in the list
list_of_dataframes_study1 <- lapply(list_of_dataframes_study1, coerce_columns_jsm) # NOTE: coerce_columns_jsm OR coerce_columns_whu
# -- 0c) Bind rows together
combined_df_study1 <- bind_rows(list_of_dataframes_study1)
# -- 0d) Aggregate data by 'agent' and 'question.id' and create 'multiple_rows_answered'
aggregated_df_study1 <- combined_df_study1 %>%
group_by(`simulation.id`, agent, `question.id`) %>%
mutate(answer = paste0("[", paste(answer, collapse = ","), "]"),
multiple_rows_answered = ifelse(n() > 1, 1, 0)) %>%
slice(1) %>%
ungroup()
# -- 0e) Remove double opening and closing square brackets
aggregated_df_study1$answer <- gsub("\\[\\[", "[", aggregated_df_study1$answer)
aggregated_df_study1$answer <- gsub("\\]\\]", "]", aggregated_df_study1$answer)
# -- 0f) Initialize columns with NA
aggregated_df_study1$list1 <- NA
aggregated_df_study1$list2 <- NA
# -- 0g) Function to handle aggregated answers
handle_aggregated_answer <- function(answer) {
# Check if answer needs to be wrapped in square brackets
if (startsWith(answer, "[") && grepl("\\],\\[", answer) && endsWith(answer, "]")) {
answer <- paste0("[", answer, "]")
}
# Try parsing, if it fails return the answer unmodified
parsed <- tryCatch({
fromJSON(answer, simplifyVector = FALSE)
}, error = function(e) NULL)
# If parsing failed or result is not a list of lists with exactly two members, return the original answer
if (is.null(parsed) || !is.list(parsed) || length(parsed) != 2 || !all(sapply(parsed, is.list))) {
return(list(answer = answer, list1 = NA, list2 = NA))
}
# Separate the two lists
list1 <- unlist(parsed[[1]])
list2 <- unlist(parsed[[2]])
# Check if the two lists have the same length
if(length(list1) != length(list2)) {
return(list(answer = answer, list1 = list1, list2 = list2))
}
# Compute the average
avg <- mapply(function(x, y) (as.numeric(x) + as.numeric(y)) / 2, list1, list2)
# Convert the average list to the desired character format
avg_string <- paste0("[", paste(avg, collapse = ", "), "]")
return(list(answer = avg_string, list1 = list1, list2 = list2))
}
# -- 0h) Apply the function to the 'answer' column, and bind the list of lists together
result_study1 <- lapply(aggregated_df_study1$answer, handle_aggregated_answer)
# -- 0i) Update the dataframe with the results
aggregated_df_study1$answer <- sapply(result_study1, function(x) x$answer)
aggregated_df_study1$list1 <- sapply(result_study1, function(x) x$list1)
aggregated_df_study1$list2 <- sapply(result_study1, function(x) x$list2)
# -- a) Parse the "answer" column # NOTE - UP TO HERE....
parsed_data_study1 <- lapply(aggregated_df_study1$answer, function(x) {
tryCatch({
fromJSON(x, simplifyVector = FALSE)
}, error = function(e) NULL)
})
# -- b) Determine the maximum length of answers across all rows
max_length_study1 <- max(sapply(parsed_data_study1, length))
# -- c) Extract answers according to the structure of each parsed object
parsed_answers_study1 <- lapply(parsed_data_study1, function(x) {extract_answers(x, max_length_study1)})
# -- d) Ensure Each "answers" is of length max_length - padded with NAs
parsed_answers_study1 <- lapply(parsed_answers_study1, function(item) {
item$answers <- c(item$answers, rep(NA, max_length_study1 - length(item$answers)))
return(item)
})
# -- e) Convert the parsed answers into a data.frame and add response_type column
# Step 1: Create the main part of answer_df_study2
answer_df_study1 <- lapply(parsed_answers_study1, function(x) {
answers <- x[["answers"]]
# Limit to max_length_study2, fill with NA if less
length(answers) <- max_length_study1
return(answers)
})
# Convert to a data frame
answer_df_study1 <- do.call(rbind, lapply(answer_df_study1, function(x) as.data.frame(t(x))))
colnames(answer_df_study1) <- paste0("answer_", seq_len(max_length_study1))
# Step 2: Create a separate column for extra answers
extra_answers <- lapply(parsed_answers_study1, function(x) {
answers <- x[["answers"]]
if(length(answers) > max_length_study1) {
return(answers[(max_length_study1 + 1):length(answers)])
} else {
return(NA)
}
})
answer_df_study1$extra_answers <- I(extra_answers) # 'I' to keep the list structure in the dataframe
# Step 3: Add response_type
response_types <- lapply(parsed_answers_study1, `[[`, "response_type")
answer_df_study1$response_type <- unlist(response_types)
# -- f) Bind the original dataframe with the answer_df
final_df_study1 <- bind_cols(aggregated_df_study1, answer_df_study1)
# Change response_type to 'two_lists' for rows where list1 or list2 are non-NA
final_df_study1$response_type[!is.na(final_df_study1$list1) | !is.na(final_df_study1$list2)] <- "two_lists"
# Function to check for list class in a column
is_list_in_column <- function(column) {
sapply(column, function(x) class(x) == "list")
}
# -- g) Extract answer instructions for each row
parsed_instructions_study1 <- lapply(aggregated_df_study1$`answer.instruction`, extract_instructions)
# -- h) Determine the max number of instructions across rows
max_instructions_study1 <- max(sapply(parsed_instructions_study1, length))
# -- i) Convert the list to a dataframe with wide format
# Step 1: Initialize a new list to store the modified instructions
modified_instructions_study1 <- vector("list", length = length(parsed_instructions_study1))
# Step 2: Ensure each element is a list of length 17
modified_instructions_study1 <- lapply(parsed_instructions_study1, function(x) {
# If the element is not a list, convert it to a list
if (is.character(x)){
x <- unlist(strsplit(x, "\\d+\\.\\s*"))
x <- unique(x[nzchar(x)])
x <- as.list(x)
} else if (!is.list(x)) {
x <- list(x)
}
# Pad the list with NAs to make its length 17
length(x) <- max_instructions_study1
return(x)
})
# Step 3:Combine all lists into a data frame
instruction_df_study1 <- do.call(rbind, modified_instructions_study1)
colnames(instruction_df_study1) <- paste0("instruction_", 1:max_instructions_study1)
# Step 4: Convert the result into a data frame
instruction_df_study1 <- as.data.frame(instruction_df_study1)
# -- j) Combine the new instruction columns with the original dataframe
final_df_study1 <- bind_cols(final_df_study1, instruction_df_study1)
# -- k1) Get the subset of columns that start with "answer_" and "instruction_" and convert to numeric and character, respectively
answer_columns_study1 <- grep("^answer_", colnames(final_df_study1), value = TRUE)
#final_df_study1[, answer_columns_study1] <- lapply(final_df_study1[, answer_columns_study1], as.numeric) # Convert those columns to numeric --> issues with list types at the moment??
instruction_columns_study1 <- grep("^instruction_", colnames(final_df_study1), value = TRUE)
final_df_study1[, instruction_columns_study1] <- lapply(final_df_study1[, instruction_columns_study1], as.character) # Convert those columns to character
# -- k2) Change the response_type where multiple_rows_answered is greater than 0, and Drop the multiple_rows_answered column
final_df_study1$response_type[final_df_study1$multiple_rows_answered > 0] <- "multiple_lines"
final_df_study1$multiple_rows_answered <- NULL
# -- li) Apply the coercion function to each dataframe in the list
final_df_study1 <- coerce_final_columns(final_df_study1)
# -- l1) Calculate the number of non-NA values for each row in the subset of columns, and save to xlsx file (OPTIONAL)
final_df_study1$non_na_counts <- apply(final_df_study1[, answer_columns_study1], MARGIN = 1, function(x) sum(!is.na(x)))
# (OPTIONAL) Count the number of occurrences for each unique value of "question.id" to check we have enough data coverage across the conditions/each experiment (i.e. greater than 500 observations, plu or minus 10%)
table(final_df_study1$question.id)
